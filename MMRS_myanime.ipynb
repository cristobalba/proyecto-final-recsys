{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "from surprise import AlgoBase\n",
    "from surprise import NormalPredictor\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import NMF\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular Popularidad de ítems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_item_popularity(df):\n",
    "    # Calcular la cantidad total de usuarios\n",
    "    total_users = df['user_id'].nunique()\n",
    "    \n",
    "    # Contar la cantidad de usuarios únicos que compraron cada item\n",
    "    item_user_count = df.groupby('item_id')['user_id'].nunique().reset_index()\n",
    "    \n",
    "    # Renombrar la columna user_id a popularity\n",
    "    item_user_count = item_user_count.rename(columns={'user_id': 'popularity'})\n",
    "    \n",
    "    # Calcular la popularidad dividiendo por la cantidad total de usuarios\n",
    "    item_user_count['popularity'] = item_user_count['popularity'] / total_users\n",
    "    \n",
    "    return item_user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular ítems más populares (top 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_percent_items(popularity_df):\n",
    "    # Ordenar los items por popularidad de mayor a menor\n",
    "    sorted_popularity_df = popularity_df.sort_values(by='popularity', ascending=False)\n",
    "    \n",
    "    # Calcular el número de ítems que corresponde al 20%\n",
    "    top_20_percent_count = int(len(sorted_popularity_df) * 0.20)\n",
    "    \n",
    "    # Obtener los ítems más populares que corresponden al 20%\n",
    "    I_pop = sorted_popularity_df.head(top_20_percent_count)['item_id'].tolist()\n",
    "    \n",
    "    return I_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular popularidad de users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_popularity(df, I_pop):\n",
    "    # Filtrar los items rateados que están en I_pop\n",
    "    df['is_popular'] = df['item_id'].isin(I_pop)\n",
    "    \n",
    "    # Calcular la cantidad de items populares rateados por cada usuario\n",
    "    user_popular_items_count = df[df['is_popular']].groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_popular_items_count = user_popular_items_count.rename(columns={'item_id': 'popular_items_count'})\n",
    "    \n",
    "    # Calcular la cantidad total de items rateados por cada usuario\n",
    "    user_total_items_count = df.groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_total_items_count = user_total_items_count.rename(columns={'item_id': 'total_items_count'})\n",
    "\n",
    "    # Asegurar que todos los usuarios están presentes en el resultado final\n",
    "    user_popular_items_count = pd.merge(user_total_items_count[['user_id']], \n",
    "                                        user_popular_items_count, \n",
    "                                        on='user_id', \n",
    "                                        how='left').fillna(0)\n",
    "\n",
    "    # Combinar los dos DataFrames\n",
    "    user_popularity_df = pd.merge(user_popular_items_count, user_total_items_count, on='user_id')\n",
    "    \n",
    "    # Calcular user_pop dividiendo popular_items_count por total_items_count\n",
    "    user_popularity_df['user_pop'] = user_popularity_df['popular_items_count'] / user_popularity_df['total_items_count']\n",
    "    \n",
    "    # Seleccionar solo las columnas user_id y user_pop\n",
    "    user_popularity_df = user_popularity_df[['user_id', 'user_pop']]\n",
    "    \n",
    "    return user_popularity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con dataset de amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv('myanime_600K.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_amazon = df_amazon[['User_id', 'Id', 'review/score']]\n",
    "#df_amazon = df_amazon.rename(columns={'User_id' : 'user_id', 'Id': 'item_id', 'review/score' : 'rating'})\n",
    "df_amazon = df_amazon.rename(columns={'anime_id': 'item_id'})\n",
    "popularity_df = calculate_item_popularity(df_amazon)\n",
    "I_pop = get_top_20_percent_items(popularity_df)\n",
    "user_popularity_df = calculate_user_popularity(df_amazon, I_pop)\n",
    "user_popularity_df = user_popularity_df.sort_values(by='user_pop', ascending=False)\n",
    "top = user_popularity_df.head(len(user_popularity_df)//3)\n",
    "bot = user_popularity_df.tail(len(user_popularity_df)//3)\n",
    "# take the 100 users in the middle of the ranking, using the len of user_popularity_df\n",
    "mid = user_popularity_df.iloc[len(user_popularity_df)//3: len(user_popularity_df) - len(user_popularity_df)//3]\n",
    "\n",
    "top = top.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#top = top.iloc[:1000]\n",
    "bot = bot.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#bot = bot.iloc[len(bot) - 1000:]\n",
    "mid = mid.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#mid = mid.iloc[len(mid)//2 - 500: len(mid)//2 + 500]\n",
    "df_amazon = df_amazon.rename(columns={'user_id': 'user', 'product_id': 'item', 'rating': 'preference'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top.to_csv('./myanime/top.csv', index=False)\n",
    "# bot.to_csv('./myanime/bot.csv', index=False)\n",
    "# mid.to_csv('./myanime/mid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 users con high pop\n",
      "      user  mainstreaminess\n",
      "2077  2370              1.0\n",
      "3581  4100              1.0\n",
      "3583  4102              1.0\n",
      "3568  4083              1.0\n",
      "3569  4084              1.0\n",
      "3592  4112              1.0\n",
      "3593  4113              1.0\n",
      "3595  4115              1.0\n",
      "780    892              1.0\n",
      "768    880              1.0\n",
      "Top 10 users con mid pop\n",
      "      user  mainstreaminess\n",
      "3376  3862         0.968109\n",
      "3373  3859         0.968085\n",
      "1939  2212         0.968085\n",
      "1775  2031         0.968085\n",
      "1757  2010         0.968000\n",
      "2901  3322         0.967949\n",
      "3472  3967         0.967949\n",
      "3490  3992         0.967890\n",
      "2124  2425         0.967871\n",
      "2663  3049         0.967836\n",
      "Top 10 users con low pop\n",
      "      user  mainstreaminess\n",
      "1236  1417         0.892063\n",
      "614    708         0.891892\n",
      "535    622         0.891892\n",
      "2041  2332         0.891753\n",
      "507    590         0.891608\n",
      "1761  2016         0.891566\n",
      "1543  1764         0.891566\n",
      "3209  3671         0.891304\n",
      "1953  2227         0.891304\n",
      "1461  1670         0.891089\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 users con high pop\")\n",
    "print(top.head(10))\n",
    "print(\"Top 10 users con mid pop\")\n",
    "print(mid.head(10))\n",
    "print(\"Top 10 users con low pop\")\n",
    "print(bot.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "dataset = 'book'#options:'lfm', anime', 'book', 'ml'\n",
    "folds = 5\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "top_fraction = 0.2\n",
    "# user_events_file = 'data/' + dataset + '/user_events.txt'\n",
    "# low_user_file = 'data/' + dataset + '/low_main_users.txt'\n",
    "# medium_user_file = 'data/' + dataset + '/medium_main_users.txt'\n",
    "# high_user_file = 'data/' + dataset + '/high_main_users.txt'\n",
    "df_events = df_amazon.copy()\n",
    "df_events = df_events[['user', 'item_id', 'preference']]\n",
    "df_events = df_events.rename(columns={'item_id': 'item'})\n",
    "low_users = bot.copy()\n",
    "low_users.set_index('user', inplace=True)\n",
    "medium_users = mid.copy()\n",
    "medium_users.set_index('user', inplace=True)\n",
    "high_users = top.copy()\n",
    "high_users.set_index('user', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 633278\n",
      "No. of users: 3614\n",
      "No. of events per user: 175.22910902047593\n"
     ]
    }
   ],
   "source": [
    "# read user events and users\n",
    "cols = ['user', 'item', 'preference']\n",
    "#df_events = pd.read_csv(user_events_file, sep=',', names=cols, skiprows=1)\n",
    "print('No. of user events: ' + str(len(df_events)))\n",
    "# read users\n",
    "#low_users = pd.read_csv(low_user_file, sep=',').set_index('user')\n",
    "#medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user')\n",
    "#high_users = pd.read_csv(high_user_file, sep=',').set_index('user')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))\n",
    "print('No. of events per user: ' + str(len(df_events) / no_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. items: 11536\n",
      "No. of events per item: 54.895804438280166\n"
     ]
    }
   ],
   "source": [
    "# get item distribution\n",
    "item_dist = df_events['item'].value_counts()\n",
    "num_items = len(item_dist)\n",
    "print('No. items: ' + str(num_items))\n",
    "# create item dataframe with normalized item counts\n",
    "df_item_dist = pd.DataFrame(item_dist)\n",
    "df_item_dist.columns = ['count']\n",
    "df_item_dist['count'] /= no_users\n",
    "print('No. of events per item: ' + str(len(df_events) / num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848102367353956"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparsity\n",
    "1 - len(df_events) / (no_users * num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min rating: 1\n",
      "Max rating: 10\n"
     ]
    }
   ],
   "source": [
    "# rating range\n",
    "print('Min rating: ' + str(df_events['preference'].min()))\n",
    "print('Max rating: ' + str(df_events['preference'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get fractions\n",
    "# user_hist = [] # user history sizes\n",
    "# pop_item_fraq = [] # average popularity of items in user profiles\n",
    "# for u, df in df_events.groupby('user'):\n",
    "#     no_user_items = len(set(df['item'])) # profile size\n",
    "#     user_hist.append(no_user_items)\n",
    "#     # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "#     user_pop_item_fraq = sum(item_dist[df['item']] / no_users) / no_user_items\n",
    "#     pop_item_fraq.append(user_pop_item_fraq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(user_hist, pop_item_fraq)\n",
    "# print('R-value: ' + str(r_value))\n",
    "# print('R2-value: ' + str(r_value**2))\n",
    "# print('P-value: ' + str(p_value))\n",
    "# print('Slope: ' + str(slope))\n",
    "# print('Intercept: ' + str(intercept))\n",
    "# print(stats.spearmanr(user_hist, pop_item_fraq))\n",
    "\n",
    "# line = slope * np.array(user_hist) + intercept\n",
    "# plt.plot(user_hist, pop_item_fraq, 'o', user_hist, line)\n",
    "# plt.xlabel('User profile size', fontsize='15')\n",
    "# plt.ylabel('Average popularity of items', fontsize='15')\n",
    "# plt.xticks(fontsize='13')\n",
    "# plt.yticks(fontsize='13')\n",
    "# #plt.savefig('data/' + dataset + '/plots/corr_user_avg.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(df_events['preference'].min(), df_events['preference'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_events, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_of_groups(predictions):\n",
    "    #print('All: ')\n",
    "    #all_mae = accuracy.mae(predictions)\n",
    "    all_predictions = []\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in low_users.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in medium_users.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))          \n",
    "    low_mae = np.mean(low_predictions)\n",
    "    #print('LowMS: ' + str(low_mae))\n",
    "    med_mae = np.mean(med_predictions)\n",
    "    #print('MedMS: ' + str(med_mae))\n",
    "    high_mae = np.mean(high_predictions)\n",
    "    #print('HighMS: ' + str(high_mae))\n",
    "    all_mae = np.mean([low_mae, med_mae, high_mae])\n",
    "    #print('All: ' + str(all_mae))\n",
    "    print('Low vs. med: ' + str(stats.ttest_ind(low_predictions, med_predictions)))\n",
    "    print('Low vs. high: ' + str(stats.ttest_ind(low_predictions, high_predictions)))\n",
    "    \n",
    "    return low_mae, med_mae, high_mae, all_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    low_precisions = []\n",
    "    med_precisions = []\n",
    "    high_precisions = []\n",
    "    low_recalls = []\n",
    "    med_recalls = []\n",
    "    high_recalls = []\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    for uid in precisions.keys():\n",
    "        if uid in low_users.index:\n",
    "            low_precisions.append(precisions[uid])\n",
    "            low_recalls.append(recalls[uid])\n",
    "        elif uid in medium_users.index:\n",
    "            med_precisions.append(precisions[uid])\n",
    "            med_recalls.append(recalls[uid])\n",
    "        else:\n",
    "            high_precisions.append(precisions[uid])\n",
    "            high_recalls.append(recalls[uid])\n",
    "    \n",
    "    return np.mean(low_precisions), np.mean(med_precisions), np.mean(high_precisions), np.mean(low_recalls), np.mean(med_recalls), np.mean(high_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNBasic\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Low Precision: 0.9938201267567784\n",
      "Med Precision: 0.9955457707767221\n",
      "High Precision: 0.994249268214902\n",
      "All Precision: 0.9945383885828007\n",
      "\n",
      "\n",
      "Low Recall: 0.39657141133273716\n",
      "Med Recall: 0.4849721559767105\n",
      "High Recall: 0.7247747820510398\n",
      "All Recall: 0.5354394497868292\n",
      "KNNWithMeans\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Low Precision: 0.9915877565683928\n",
      "Med Precision: 0.9955473982119647\n",
      "High Precision: 0.9942943911602035\n",
      "All Precision: 0.9938098486468536\n",
      "\n",
      "\n",
      "Low Recall: 0.39613806776898836\n",
      "Med Recall: 0.48478519778438856\n",
      "High Recall: 0.7247455270760168\n",
      "All Recall: 0.5352229308764646\n",
      "NMF\n",
      "Low Precision: 0.9925776793395326\n",
      "Med Precision: 0.9952142907133863\n",
      "High Precision: 0.9939831473956422\n",
      "All Precision: 0.9939250391495204\n",
      "\n",
      "\n",
      "Low Recall: 0.3960976163465185\n",
      "Med Recall: 0.48452718107564297\n",
      "High Recall: 0.7244137077574455\n",
      "All Recall: 0.5350128350598691\n",
      "CoClustering\n",
      "Low Precision: 0.9917593204269437\n",
      "Med Precision: 0.9956996481792876\n",
      "High Precision: 0.9943917621678924\n",
      "All Precision: 0.9939502435913747\n",
      "\n",
      "\n",
      "Low Recall: 0.3960974193955978\n",
      "Med Recall: 0.48478724212146407\n",
      "High Recall: 0.7246998161380069\n",
      "All Recall: 0.535194825885023\n"
     ]
    }
   ],
   "source": [
    "sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "algos = []\n",
    "\n",
    "algos.append(KNNBasic(sim_options = sim_users, k=40)) \n",
    "algos.append(KNNWithMeans(sim_options = sim_users, k=40))\n",
    "algos.append(NMF(n_factors = 30, random_state=my_seed))\n",
    "algos.append(CoClustering(n_cltr_u=3, n_cltr_i=3, random_state=my_seed))\n",
    "algo_names = ['KNNBasic',\n",
    "              'KNNWithMeans',\n",
    "              'NMF',\n",
    "              'CoClustering']\n",
    "\n",
    "kf = KFold(n_splits=folds, random_state = my_seed)\n",
    "for i in range(0, len(algo_names)):\n",
    "    df_item_dist[algo_names[i]] = 0\n",
    "    # low_maes = []\n",
    "    # med_maes = []\n",
    "    # high_maes = []\n",
    "    # all_maes = []\n",
    "    low_precisions = []\n",
    "    med_precisions = []\n",
    "    high_precisions = []\n",
    "    all_precisions = []\n",
    "    low_recalls = []\n",
    "    med_recalls = []\n",
    "    high_recalls = []\n",
    "    all_recalls = []\n",
    "\n",
    "    print(algo_names[i])\n",
    "    fold_count = 0\n",
    "    for trainset, testset in kf.split(data):\n",
    "        # calculate and evaluate recommendations\n",
    "        algos[i].fit(trainset)\n",
    "        predictions = algos[i].test(testset)        \n",
    "        # low_mae, med_mae, high_mae, all_mae = get_mae_of_groups(predictions)\n",
    "        # low_maes.append(low_mae)\n",
    "        # med_maes.append(med_mae)\n",
    "        # high_maes.append(high_mae)\n",
    "        # all_maes.append(all_mae)\n",
    "\n",
    "        # calculate precision and recall\n",
    "        low_precision, mid_precision, high_precision, low_recall, mid_recall, high_recall = precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
    "        low_precisions.append(low_precision)\n",
    "        med_precisions.append(mid_precision)\n",
    "        high_precisions.append(high_precision)\n",
    "        all_precisions.append(np.mean([low_precision, mid_precision, high_precision]))\n",
    "\n",
    "        low_recalls.append(low_recall)\n",
    "        med_recalls.append(mid_recall)\n",
    "        high_recalls.append(high_recall)\n",
    "        all_recalls.append(np.mean([low_recall, mid_recall, high_recall]))\n",
    "\n",
    "        # get top-n recommendation counts\n",
    "        # top_n = get_top_n(predictions, n=10)\n",
    "        # for uid, user_ratings in top_n.items():\n",
    "        #     for (iid, _) in user_ratings:\n",
    "        #         df_item_dist.loc[iid, algo_names[i]] += 1\n",
    "        \n",
    "    # print('LowMS: ' + str(np.mean(low_maes)))\n",
    "    # print('MedMS: ' + str(np.mean(med_maes)))\n",
    "    # print('HighMS: ' + str(np.mean(high_maes)))\n",
    "    # print('All: ' + str(np.mean(all_maes)))\n",
    "    print('Low Precision: ' + str(np.mean(low_precisions)))\n",
    "    print('Med Precision: ' + str(np.mean(med_precisions)))\n",
    "    print('High Precision: ' + str(np.mean(high_precisions)))\n",
    "    print('All Precision: ' + str(np.mean(all_precisions)))\n",
    "    print('\\n')\n",
    "    print('Low Recall: ' + str(np.mean(low_recalls)))\n",
    "    print('Med Recall: ' + str(np.mean(med_recalls)))\n",
    "    print('High Recall: ' + str(np.mean(high_recalls)))\n",
    "    print('All Recall: ' + str(np.mean(all_recalls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNBasic\n",
    "* Low Precision: 0.9938201267567784\n",
    "* Med Precision: 0.9955457707767221\n",
    "* High Precision: 0.994249268214902\n",
    "* All Precision: 0.9945383885828007\n",
    "\n",
    "* Low Recall: 0.39657141133273716\n",
    "* Med Recall: 0.4849721559767105\n",
    "* High Recall: 0.7247747820510398\n",
    "* All Recall: 0.5354394497868292\n",
    "\n",
    "KNNWithMeans\n",
    "* Low Precision: 0.9915877565683928\n",
    "* Med Precision: 0.9955473982119647\n",
    "* High Precision: 0.9942943911602035\n",
    "* All Precision: 0.9938098486468536\n",
    "\n",
    "* Low Recall: 0.39613806776898836\n",
    "* Med Recall: 0.48478519778438856\n",
    "* High Recall: 0.7247455270760168\n",
    "* All Recall: 0.5352229308764646\n",
    "\n",
    "NMF\n",
    "* Low Precision: 0.9925776793395326\n",
    "* Med Precision: 0.9952142907133863\n",
    "* High Precision: 0.9939831473956422\n",
    "* All Precision: 0.9939250391495204\n",
    "\n",
    "* Low Recall: 0.3960976163465185\n",
    "* Med Recall: 0.48452718107564297\n",
    "* High Recall: 0.7244137077574455\n",
    "* All Recall: 0.5350128350598691\n",
    "\n",
    "CoClustering\n",
    "* Low Precision: 0.9917593204269437\n",
    "* Med Precision: 0.9956996481792876\n",
    "* High Precision: 0.9943917621678924\n",
    "* All Precision: 0.9939502435913747\n",
    "\n",
    "* Low Recall: 0.3960974193955978\n",
    "* Med Recall: 0.48478724212146407\n",
    "* High Recall: 0.7246998161380069\n",
    "* All Recall: 0.535194825885023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
