{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install implicit --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install surprise --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbenavidesa/.local/lib/python3.10/site-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: no CUDA-capable device is detected (/project/./implicit/gpu/utils.h:71)'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular Popularidad de ítems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_item_popularity(df):\n",
    "    # Calcular la cantidad total de usuarios\n",
    "    total_users = df['user_id'].nunique()\n",
    "    \n",
    "    # Contar la cantidad de usuarios únicos que compraron cada item\n",
    "    item_user_count = df.groupby('item_id')['user_id'].nunique().reset_index()\n",
    "    \n",
    "    # Renombrar la columna user_id a popularity\n",
    "    item_user_count = item_user_count.rename(columns={'user_id': 'popularity'})\n",
    "    \n",
    "    # Calcular la popularidad dividiendo por la cantidad total de usuarios\n",
    "    item_user_count['popularity'] = item_user_count['popularity'] / total_users\n",
    "    \n",
    "    return item_user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular ítems más populares (top 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_percent_items(popularity_df):\n",
    "    # Ordenar los items por popularidad de mayor a menor\n",
    "    sorted_popularity_df = popularity_df.sort_values(by='popularity', ascending=False)\n",
    "    \n",
    "    # Calcular el número de ítems que corresponde al 20%\n",
    "    top_20_percent_count = int(len(sorted_popularity_df) * 0.20)\n",
    "    \n",
    "    # Obtener los ítems más populares que corresponden al 20%\n",
    "    I_pop = sorted_popularity_df.head(top_20_percent_count)['item_id'].tolist()\n",
    "    \n",
    "    return I_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular popularidad de users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_popularity(df, I_pop):\n",
    "    # Filtrar los items rateados que están en I_pop\n",
    "    df['is_popular'] = df['item_id'].isin(I_pop)\n",
    "    \n",
    "    # Calcular la cantidad de items populares rateados por cada usuario\n",
    "    user_popular_items_count = df[df['is_popular']].groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_popular_items_count = user_popular_items_count.rename(columns={'item_id': 'popular_items_count'})\n",
    "    \n",
    "    # Calcular la cantidad total de items rateados por cada usuario\n",
    "    user_total_items_count = df.groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_total_items_count = user_total_items_count.rename(columns={'item_id': 'total_items_count'})\n",
    "\n",
    "    # Asegurar que todos los usuarios están presentes en el resultado final\n",
    "    user_popular_items_count = pd.merge(user_total_items_count[['user_id']], \n",
    "                                        user_popular_items_count, \n",
    "                                        on='user_id', \n",
    "                                        how='left').fillna(0)\n",
    "\n",
    "    # Combinar los dos DataFrames\n",
    "    user_popularity_df = pd.merge(user_popular_items_count, user_total_items_count, on='user_id')\n",
    "    \n",
    "    # Calcular user_pop dividiendo popular_items_count por total_items_count\n",
    "    user_popularity_df['user_pop'] = user_popularity_df['popular_items_count'] / user_popularity_df['total_items_count']\n",
    "    \n",
    "    # Seleccionar solo las columnas user_id y user_pop\n",
    "    user_popularity_df = user_popularity_df[['user_id', 'user_pop']]\n",
    "    \n",
    "    return user_popularity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar data original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "dataset = 'book'  #options:'lfm', anime', 'book', 'ml'\n",
    "folds = 5\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "top_fraction = 0.2\n",
    "user_events_file = dataset + '/user_events.txt'\n",
    "low_user_file = dataset + '/low_main_users.txt'\n",
    "medium_user_file = dataset + '/medium_main_users.txt'\n",
    "high_user_file = dataset + '/high_main_users.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read user events and users\n",
    "# df_events = pd.read_csv(user_events_file, sep=',')\n",
    "# df_events = df_events.rename(columns={'user': 'user_id', 'item': 'item_id', 'preference': 'rating'})\n",
    "# print('No. of user events: ' + str(len(df_events)))\n",
    "# # read users\n",
    "# low_users = pd.read_csv(low_user_file, sep=',').set_index('user')\n",
    "# medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user')\n",
    "# high_users = pd.read_csv(high_user_file, sep=',').set_index('user')\n",
    "# no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "# print('No. of users: ' + str(no_users))\n",
    "# print('No. of events per user: ' + str(len(df_events) / no_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rating range\n",
    "# print('Min rating: ' + str(df_events['rating'].min()))\n",
    "# print('Max rating: ' + str(df_events['rating'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar netflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 459514\n",
      "No. of users: 3000\n",
      "No. of events per user: 153.17133333333334\n"
     ]
    }
   ],
   "source": [
    "# read user events and users\n",
    "df_events = pd.read_csv('./netflix/netflix.csv', sep=',')\n",
    "# Preprocess the data\n",
    "print('No. of user events: ' + str(len(df_events)))\n",
    "# read users\n",
    "low_users = pd.read_csv('./netflix/bot.csv', sep=',').set_index('user_id')\n",
    "medium_users = pd.read_csv('./netflix/mid.csv', sep=',').set_index('user_id')\n",
    "high_users = pd.read_csv('./netflix/top.csv', sep=',').set_index('user_id')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))\n",
    "print('No. of events per user: ' + str(len(df_events) / no_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicializar new anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read user events and users\n",
    "# df_events = pd.read_csv('./myanime_600K.csv', sep=',')\n",
    "# df_events = df_events.rename(columns={'anime_id': 'item_id'})\n",
    "# # Preprocess the data\n",
    "# print('No. of user events: ' + str(len(df_events)))\n",
    "# # read users\n",
    "# low_users = pd.read_csv('./myanime/bot.csv', sep=',').set_index('user')\n",
    "# medium_users = pd.read_csv('./myanime/mid.csv', sep=',').set_index('user')\n",
    "# high_users = pd.read_csv('./myanime/top.csv', sep=',').set_index('user')\n",
    "# no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "# print('No. of users: ' + str(no_users))\n",
    "# print('No. of events per user: ' + str(len(df_events) / no_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_events.copy()\n",
    "\n",
    "# Ajustar segun el dataset\n",
    "\n",
    "# Myanime y book crossing x = 1 si rating >= 6, x = 0 si rating < 6\n",
    "#df_events['rating'] = df_events['rating'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Netflix y movieLens x = 1 si rating >= 3, x = 0 si rating < 3\n",
    "df_events['rating'] = df_events['rating'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "# LFM x = 1 si rating >= 60, x = 0 si rating < 60\n",
    "#df_events['rating'] = df_events['rating'].apply(lambda x: 1 if x >= 60 else 0)\n",
    "\n",
    "# Reindexar usuarios e ítems\n",
    "df['user_id'] = df['user_id'].astype('category')\n",
    "user_cat_mapping = df['user_id'].cat.categories\n",
    "df['user_id'] = df['user_id'].cat.codes\n",
    "df['item_id'] = df['item_id'].astype('category')\n",
    "item_cat_mapping = df['item_id'].cat.categories\n",
    "df['item_id'] = df['item_id'].cat.codes\n",
    "\n",
    "# Create a sparse matrix for ALS\n",
    "sparse_item_user = coo_matrix((df['rating'].astype(float),\n",
    "                               (df['item_id'], df['user_id'])))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a sparse matrix for ALS training\n",
    "sparse_item_user_train = coo_matrix((train_data['rating'].astype(float),\n",
    "                                     (train_data['item_id'], train_data['user_id'])))\n",
    "\n",
    "# Create a sparse matrix for ALS test\n",
    "sparse_item_user_test = coo_matrix((test_data['rating'].astype(float),\n",
    "                                    (test_data['item_id'], test_data['user_id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06fdb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cbenavidesa/.local/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "/home/cbenavidesa/.local/lib/python3.10/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.01369929313659668 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa31cfb1c79a456eb9b911fbb01686e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the ALS model\n",
    "model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "\n",
    "# Train the ALS model\n",
    "model.fit(sparse_item_user_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([2598097,  847615, 1770582,  453497,   67188, 1638715, 1131687, 2251190,\n",
       "        315988, 1602731,\n",
       "       ...\n",
       "        281628,  770204, 1541981, 1533580,  989589, 2263361, 1870457,  616769,\n",
       "        844639, 1140339],\n",
       "      dtype='int64', name='user_id', length=1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_users.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(model, user_id, sparse_item_user, N=10):\n",
    "    user_items = sparse_item_user.T.tocsr()\n",
    "    recommendations = model.recommend(user_id, user_items[user_id], N=N)\n",
    "    return recommendations[0], recommendations[1]\n",
    "\n",
    "\n",
    "# Define evaluation metrics\n",
    "def precision_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r):\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "def recall_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    return np.sum(r) / len(r)\n",
    "\n",
    "\n",
    "# Create user-item interaction dictionaries for test data\n",
    "user_items_test = test_data.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, user_items_test, sparse_item_user, n=10):\n",
    "    mean_pre_low = 0.\n",
    "    mean_pre_med = 0.\n",
    "    mean_pre_high = 0.\n",
    "    mean_recall_low = 0.\n",
    "    mean_recall_med = 0.\n",
    "    mean_recall_high = 0.\n",
    "    user_low = 0\n",
    "    user_med = 0\n",
    "    user_high = 0\n",
    "\n",
    "    user_items = sparse_item_user.T.tocsr()\n",
    "    \n",
    "    for user in user_items_test.keys():\n",
    "        rec_items, _ = recommend(model, user, user_items, N=n)\n",
    "        rel_vector = [1 if item in user_items_test[user] else 0 for item in rec_items]\n",
    "        user_original_id = user_cat_mapping[user]\n",
    "        if user_original_id in low_users.index:\n",
    "            mean_pre_low += precision_at_k(rel_vector, n)\n",
    "            mean_recall_low += recall_at_k(rel_vector, n)\n",
    "            user_low += 1\n",
    "        elif user_original_id in medium_users.index:\n",
    "            mean_pre_med += precision_at_k(rel_vector, n)\n",
    "            mean_recall_med += recall_at_k(rel_vector, n)\n",
    "            user_med += 1\n",
    "        else:\n",
    "            mean_pre_high += precision_at_k(rel_vector, n)\n",
    "            mean_recall_high += recall_at_k(rel_vector, n)\n",
    "            user_high += 1\n",
    "    mean_pre_low /= user_low\n",
    "    mean_pre_med /= user_med\n",
    "    mean_pre_high /= user_high\n",
    "    mean_recall_low /= user_low\n",
    "    mean_recall_med /= user_med\n",
    "    mean_recall_high /= user_high\n",
    "    print(user_low, user_med, user_high)\n",
    "    \n",
    "    return mean_pre_low, mean_pre_med, mean_pre_high, mean_recall_low, mean_recall_med, mean_recall_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967 995 939\n",
      "Precision Low: 0.07776628748707362\n",
      "Precision Med: 0.10140703517587903\n",
      "Precision High: 0.0935037273695419\n",
      "Recall Low: 0.07776628748707362\n",
      "Recall Med: 0.10140703517587903\n",
      "Recall High: 0.0935037273695419\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform the evaluation\n",
    "mean_pre_low, mean_pre_med, mean_pre_high, mean_recall_low, mean_recall_med, mean_recall_high = evaluate_model(model, user_items_test, sparse_item_user_test, n=10)\n",
    "print('Precision Low: ' + str(mean_pre_low))\n",
    "print('Precision Med: ' + str(mean_pre_med))\n",
    "print('Precision High: ' + str(mean_pre_high))\n",
    "print('Recall Low: ' + str(mean_recall_low))\n",
    "print('Recall Med: ' + str(mean_recall_med))\n",
    "print('Recall High: ' + str(mean_recall_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.07776628748707362\n",
    "Precision Med: 0.10140703517587903\n",
    "Precision High: 0.0935037273695419\n",
    "Recall Low: 0.07776628748707362\n",
    "Recall Med: 0.10140703517587903\n",
    "Recall High: 0.0935037273695419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.144274809160304\n",
    "Precision Med: 0.14181213632585082\n",
    "Precision High: 0.12122641509433925\n",
    "Recall Low: 0.144274809160304\n",
    "Recall Med: 0.14181213632585082\n",
    "Recall High: 0.12122641509433925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.13009999999999955\n",
    "Precision Med: 0.18159999999999885\n",
    "Precision High: 0.1783999999999989\n",
    "Recall Low: 0.13009999999999955\n",
    "Recall Med: 0.18159999999999885\n",
    "Recall High: 0.1783999999999989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.15359999999999926\n",
    "Precision Med: 0.15359999999999918\n",
    "Precision High: 0.14369999999999913\n",
    "Recall Low: 0.15359999999999926\n",
    "Recall Med: 0.15359999999999918\n",
    "Recall High: 0.14369999999999913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.020000000000000004\n",
    "Precision Med: 0.03950000000000016\n",
    "Precision High: 0.04550000000000023\n",
    "Recall Low: 0.020000000000000004\n",
    "Recall Med: 0.03950000000000016\n",
    "Recall High: 0.04550000000000023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision Low: 0.1236999999999996\n",
    "Precision Med: 0.1258999999999995\n",
    "Precision High: 0.12379999999999945\n",
    "Recall Low: 0.1236999999999996\n",
    "Recall Med: 0.1258999999999995\n",
    "Recall High: 0.12379999999999945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
