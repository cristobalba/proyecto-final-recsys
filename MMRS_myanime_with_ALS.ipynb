{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "\n",
    "from surprise import AlgoBase\n",
    "from surprise import NormalPredictor\n",
    "from surprise import BaselineOnly\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNBaseline\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import NMF\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular Popularidad de ítems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_item_popularity(df):\n",
    "    # Calcular la cantidad total de usuarios\n",
    "    total_users = df['user_id'].nunique()\n",
    "    \n",
    "    # Contar la cantidad de usuarios únicos que compraron cada item\n",
    "    item_user_count = df.groupby('item_id')['user_id'].nunique().reset_index()\n",
    "    \n",
    "    # Renombrar la columna user_id a popularity\n",
    "    item_user_count = item_user_count.rename(columns={'user_id': 'popularity'})\n",
    "    \n",
    "    # Calcular la popularidad dividiendo por la cantidad total de usuarios\n",
    "    item_user_count['popularity'] = item_user_count['popularity'] / total_users\n",
    "    \n",
    "    return item_user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular ítems más populares (top 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_20_percent_items(popularity_df):\n",
    "    # Ordenar los items por popularidad de mayor a menor\n",
    "    sorted_popularity_df = popularity_df.sort_values(by='popularity', ascending=False)\n",
    "    \n",
    "    # Calcular el número de ítems que corresponde al 20%\n",
    "    top_20_percent_count = int(len(sorted_popularity_df) * 0.20)\n",
    "    \n",
    "    # Obtener los ítems más populares que corresponden al 20%\n",
    "    I_pop = sorted_popularity_df.head(top_20_percent_count)['item_id'].tolist()\n",
    "    \n",
    "    return I_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular popularidad de users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_popularity(df, I_pop):\n",
    "    # Filtrar los items rateados que están en I_pop\n",
    "    df['is_popular'] = df['item_id'].isin(I_pop)\n",
    "    \n",
    "    # Calcular la cantidad de items populares rateados por cada usuario\n",
    "    user_popular_items_count = df[df['is_popular']].groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_popular_items_count = user_popular_items_count.rename(columns={'item_id': 'popular_items_count'})\n",
    "    \n",
    "    # Calcular la cantidad total de items rateados por cada usuario\n",
    "    user_total_items_count = df.groupby('user_id')['item_id'].count().reset_index()\n",
    "    user_total_items_count = user_total_items_count.rename(columns={'item_id': 'total_items_count'})\n",
    "\n",
    "    # Asegurar que todos los usuarios están presentes en el resultado final\n",
    "    user_popular_items_count = pd.merge(user_total_items_count[['user_id']], \n",
    "                                        user_popular_items_count, \n",
    "                                        on='user_id', \n",
    "                                        how='left').fillna(0)\n",
    "\n",
    "    # Combinar los dos DataFrames\n",
    "    user_popularity_df = pd.merge(user_popular_items_count, user_total_items_count, on='user_id')\n",
    "    \n",
    "    # Calcular user_pop dividiendo popular_items_count por total_items_count\n",
    "    user_popularity_df['user_pop'] = user_popularity_df['popular_items_count'] / user_popularity_df['total_items_count']\n",
    "    \n",
    "    # Seleccionar solo las columnas user_id y user_pop\n",
    "    user_popularity_df = user_popularity_df[['user_id', 'user_pop']]\n",
    "    \n",
    "    return user_popularity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba con dataset de amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amazon = pd.read_csv('data/myanime_600K.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_amazon = df_amazon[['User_id', 'Id', 'review/score']]\n",
    "#df_amazon = df_amazon.rename(columns={'User_id' : 'user_id', 'Id': 'item_id', 'review/score' : 'rating'})\n",
    "df_amazon = df_amazon.rename(columns={'anime_id': 'item_id'})\n",
    "popularity_df = calculate_item_popularity(df_amazon)\n",
    "I_pop = get_top_20_percent_items(popularity_df)\n",
    "user_popularity_df = calculate_user_popularity(df_amazon, I_pop)\n",
    "user_popularity_df = user_popularity_df.sort_values(by='user_pop', ascending=False)\n",
    "top = user_popularity_df.head(len(user_popularity_df)//3)\n",
    "bot = user_popularity_df.tail(len(user_popularity_df)//3)\n",
    "# take the 100 users in the middle of the ranking, using the len of user_popularity_df\n",
    "mid = user_popularity_df.iloc[len(user_popularity_df)//3: len(user_popularity_df) - len(user_popularity_df)//3]\n",
    "\n",
    "top = top.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#top = top.iloc[:1000]\n",
    "bot = bot.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#bot = bot.iloc[len(bot) - 1000:]\n",
    "mid = mid.rename(columns={'user_id': 'user', 'user_pop': 'mainstreaminess'})\n",
    "#mid = mid.iloc[len(mid)//2 - 500: len(mid)//2 + 500]\n",
    "df_amazon = df_amazon.rename(columns={'user_id': 'user', 'product_id': 'item', 'rating': 'preference'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top.to_csv('./myanime/top.csv', index=False)\n",
    "# bot.to_csv('./myanime/bot.csv', index=False)\n",
    "# mid.to_csv('./myanime/mid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 users con high pop\n",
      "      user  mainstreaminess\n",
      "2077  2370              1.0\n",
      "3581  4100              1.0\n",
      "3583  4102              1.0\n",
      "3568  4083              1.0\n",
      "3569  4084              1.0\n",
      "3592  4112              1.0\n",
      "3593  4113              1.0\n",
      "3595  4115              1.0\n",
      "780    892              1.0\n",
      "768    880              1.0\n",
      "Top 10 users con mid pop\n",
      "      user  mainstreaminess\n",
      "3376  3862         0.968109\n",
      "3373  3859         0.968085\n",
      "1939  2212         0.968085\n",
      "1775  2031         0.968085\n",
      "1757  2010         0.968000\n",
      "2901  3322         0.967949\n",
      "3472  3967         0.967949\n",
      "3490  3992         0.967890\n",
      "2124  2425         0.967871\n",
      "2663  3049         0.967836\n",
      "Top 10 users con low pop\n",
      "      user  mainstreaminess\n",
      "1236  1417         0.892063\n",
      "614    708         0.891892\n",
      "535    622         0.891892\n",
      "2041  2332         0.891753\n",
      "507    590         0.891608\n",
      "1761  2016         0.891566\n",
      "1543  1764         0.891566\n",
      "3209  3671         0.891304\n",
      "1953  2227         0.891304\n",
      "1461  1670         0.891089\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 users con high pop\")\n",
    "print(top.head(10))\n",
    "print(\"Top 10 users con mid pop\")\n",
    "print(mid.head(10))\n",
    "print(\"Top 10 users con low pop\")\n",
    "print(bot.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and initialization\n",
    "dataset = 'book'#options:'lfm', anime', 'book', 'ml'\n",
    "folds = 5\n",
    "my_seed = 0\n",
    "rd.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "top_fraction = 0.2\n",
    "# user_events_file = 'data/' + dataset + '/user_events.txt'\n",
    "# low_user_file = 'data/' + dataset + '/low_main_users.txt'\n",
    "# medium_user_file = 'data/' + dataset + '/medium_main_users.txt'\n",
    "# high_user_file = 'data/' + dataset + '/high_main_users.txt'\n",
    "df_events = df_amazon.copy()\n",
    "df_events = df_events[['user', 'item_id', 'preference']]\n",
    "df_events = df_events.rename(columns={'item_id': 'item'})\n",
    "low_users = bot.copy()\n",
    "low_users.set_index('user', inplace=True)\n",
    "medium_users = mid.copy()\n",
    "medium_users.set_index('user', inplace=True)\n",
    "high_users = top.copy()\n",
    "high_users.set_index('user', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of user events: 633278\n",
      "No. of users: 3614\n",
      "No. of events per user: 175.22910902047593\n"
     ]
    }
   ],
   "source": [
    "# read user events and users\n",
    "cols = ['user', 'item', 'preference']\n",
    "#df_events = pd.read_csv(user_events_file, sep=',', names=cols, skiprows=1)\n",
    "print('No. of user events: ' + str(len(df_events)))\n",
    "# read users\n",
    "#low_users = pd.read_csv(low_user_file, sep=',').set_index('user')\n",
    "#medium_users = pd.read_csv(medium_user_file, sep=',').set_index('user')\n",
    "#high_users = pd.read_csv(high_user_file, sep=',').set_index('user')\n",
    "no_users = len(low_users) + len(medium_users) + len(high_users)\n",
    "print('No. of users: ' + str(no_users))\n",
    "print('No. of events per user: ' + str(len(df_events) / no_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. items: 11536\n",
      "No. of events per item: 54.895804438280166\n"
     ]
    }
   ],
   "source": [
    "# get item distribution\n",
    "item_dist = df_events['item'].value_counts()\n",
    "num_items = len(item_dist)\n",
    "print('No. items: ' + str(num_items))\n",
    "# create item dataframe with normalized item counts\n",
    "df_item_dist = pd.DataFrame(item_dist)\n",
    "df_item_dist.columns = ['count']\n",
    "df_item_dist['count'] /= no_users\n",
    "print('No. of events per item: ' + str(len(df_events) / num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9848102367353956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparsity\n",
    "1 - len(df_events) / (no_users * num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min rating: 1\n",
      "Max rating: 10\n"
     ]
    }
   ],
   "source": [
    "# rating range\n",
    "print('Min rating: ' + str(df_events['preference'].min()))\n",
    "print('Max rating: ' + str(df_events['preference'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get fractions\n",
    "# user_hist = [] # user history sizes\n",
    "# pop_item_fraq = [] # average popularity of items in user profiles\n",
    "# for u, df in df_events.groupby('user'):\n",
    "#     no_user_items = len(set(df['item'])) # profile size\n",
    "#     user_hist.append(no_user_items)\n",
    "#     # get popularity (= fraction of users interacted with item) of user items and calculate average of it\n",
    "#     user_pop_item_fraq = sum(item_dist[df['item']] / no_users) / no_user_items\n",
    "#     pop_item_fraq.append(user_pop_item_fraq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(user_hist, pop_item_fraq)\n",
    "# print('R-value: ' + str(r_value))\n",
    "# print('R2-value: ' + str(r_value**2))\n",
    "# print('P-value: ' + str(p_value))\n",
    "# print('Slope: ' + str(slope))\n",
    "# print('Intercept: ' + str(intercept))\n",
    "# print(stats.spearmanr(user_hist, pop_item_fraq))\n",
    "\n",
    "# line = slope * np.array(user_hist) + intercept\n",
    "# plt.plot(user_hist, pop_item_fraq, 'o', user_hist, line)\n",
    "# plt.xlabel('User profile size', fontsize='15')\n",
    "# plt.ylabel('Average popularity of items', fontsize='15')\n",
    "# plt.xticks(fontsize='13')\n",
    "# plt.yticks(fontsize='13')\n",
    "# #plt.savefig('data/' + dataset + '/plots/corr_user_avg.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(df_events['preference'].min(), df_events['preference'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df_events, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae_of_groups(predictions):\n",
    "    #print('All: ')\n",
    "    #all_mae = accuracy.mae(predictions)\n",
    "    all_predictions = []\n",
    "    low_predictions = []\n",
    "    med_predictions = []\n",
    "    high_predictions = []\n",
    "    for uid, iid, true_r, est, details in predictions:\n",
    "        prediction = [(uid, iid, true_r, est, details)]\n",
    "        if uid in low_users.index:\n",
    "            low_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        elif uid in medium_users.index:\n",
    "            med_predictions.append(accuracy.mae(prediction, verbose=False))\n",
    "        else:\n",
    "            high_predictions.append(accuracy.mae(prediction, verbose=False))          \n",
    "    low_mae = np.mean(low_predictions)\n",
    "    #print('LowMS: ' + str(low_mae))\n",
    "    med_mae = np.mean(med_predictions)\n",
    "    #print('MedMS: ' + str(med_mae))\n",
    "    high_mae = np.mean(high_predictions)\n",
    "    #print('HighMS: ' + str(high_mae))\n",
    "    all_mae = np.mean([low_mae, med_mae, high_mae])\n",
    "    #print('All: ' + str(all_mae))\n",
    "    print('Low vs. med: ' + str(stats.ttest_ind(low_predictions, med_predictions)))\n",
    "    print('Low vs. high: ' + str(stats.ttest_ind(low_predictions, high_predictions)))\n",
    "    \n",
    "    return low_mae, med_mae, high_mae, all_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    low_precisions = []\n",
    "    med_precisions = []\n",
    "    high_precisions = []\n",
    "    low_recalls = []\n",
    "    med_recalls = []\n",
    "    high_recalls = []\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    for uid in precisions.keys():\n",
    "        if uid in low_users.index:\n",
    "            low_precisions.append(precisions[uid])\n",
    "            low_recalls.append(recalls[uid])\n",
    "        elif uid in medium_users.index:\n",
    "            med_precisions.append(precisions[uid])\n",
    "            med_recalls.append(recalls[uid])\n",
    "        else:\n",
    "            high_precisions.append(precisions[uid])\n",
    "            high_recalls.append(recalls[uid])\n",
    "    \n",
    "    return np.mean(low_precisions), np.mean(med_precisions), np.mean(high_precisions), np.mean(low_recalls), np.mean(med_recalls), np.mean(high_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNNBasic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Low Precision: 0.9938201267567784\n",
      "Med Precision: 0.9955457707767221\n",
      "High Precision: 0.994249268214902\n",
      "All Precision: 0.9945383885828007\n",
      "\n",
      "\n",
      "Low Recall: 0.39657141133273716\n",
      "Med Recall: 0.4849721559767105\n",
      "High Recall: 0.7247747820510398\n",
      "All Recall: 0.5354394497868292\n",
      "KNNWithMeans\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trainset, testset \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(data):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# calculate and evaluate recommendations\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     algos[i]\u001b[38;5;241m.\u001b[39mfit(trainset)\n\u001b[0;32m---> 34\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# low_mae, med_mae, high_mae, all_mae = get_mae_of_groups(predictions)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# low_maes.append(low_mae)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# med_maes.append(med_mae)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# calculate precision and recall\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     low_precision, mid_precision, high_precision, low_recall, mid_recall, high_recall \u001b[38;5;241m=\u001b[39m precision_recall_at_k(predictions, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.5\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/surprise/prediction_algorithms/algo_base.py:160\u001b[0m, in \u001b[0;36mAlgoBase.test\u001b[0;34m(self, testset, verbose)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(uid, iid, r_ui_trans, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/surprise/prediction_algorithms/algo_base.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ui_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[1;32m    163\u001b[0m ]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/surprise/prediction_algorithms/algo_base.py:102\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[0;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[1;32m    100\u001b[0m details \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miuid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miiid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# If the details dict was also returned\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/surprise/prediction_algorithms/knns.py:192\u001b[0m, in \u001b[0;36mKNNWithMeans.estimate\u001b[0;34m(self, u, i)\u001b[0m\n\u001b[1;32m    189\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mswitch(u, i)\n\u001b[1;32m    191\u001b[0m neighbors \u001b[38;5;241m=\u001b[39m [(x2, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim[x, x2], r) \u001b[38;5;28;01mfor\u001b[39;00m (x2, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myr[y]]\n\u001b[0;32m--> 192\u001b[0m k_neighbors \u001b[38;5;241m=\u001b[39m \u001b[43mheapq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlargest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans[x]\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# compute weighted average\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/heapq.py:573\u001b[0m, in \u001b[0;36mnlargest\u001b[0;34m(n, iterable, key)\u001b[0m\n\u001b[1;32m    571\u001b[0m k \u001b[38;5;241m=\u001b[39m key(elem)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top \u001b[38;5;241m<\u001b[39m k:\n\u001b[0;32m--> 573\u001b[0m     \u001b[43m_heapreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m     top, _order, _elem \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    575\u001b[0m     order \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sim_users = {'name': 'cosine', 'user_based': True}  # compute cosine similarities between users\n",
    "algos = []\n",
    "\n",
    "algos.append(KNNBasic(sim_options = sim_users, k=40)) \n",
    "algos.append(KNNWithMeans(sim_options = sim_users, k=40))\n",
    "algos.append(NMF(n_factors = 30, random_state=my_seed))\n",
    "algos.append(CoClustering(n_cltr_u=3, n_cltr_i=3, random_state=my_seed))\n",
    "algo_names = ['KNNBasic',\n",
    "              'KNNWithMeans',\n",
    "              'NMF',\n",
    "              'CoClustering']\n",
    "\n",
    "kf = KFold(n_splits=folds, random_state = my_seed)\n",
    "for i in range(0, len(algo_names)):\n",
    "    df_item_dist[algo_names[i]] = 0\n",
    "    # low_maes = []\n",
    "    # med_maes = []\n",
    "    # high_maes = []\n",
    "    # all_maes = []\n",
    "    low_precisions = []\n",
    "    med_precisions = []\n",
    "    high_precisions = []\n",
    "    all_precisions = []\n",
    "    low_recalls = []\n",
    "    med_recalls = []\n",
    "    high_recalls = []\n",
    "    all_recalls = []\n",
    "\n",
    "    print(algo_names[i])\n",
    "    fold_count = 0\n",
    "    for trainset, testset in kf.split(data):\n",
    "        # calculate and evaluate recommendations\n",
    "        algos[i].fit(trainset)\n",
    "        predictions = algos[i].test(testset)        \n",
    "        # low_mae, med_mae, high_mae, all_mae = get_mae_of_groups(predictions)\n",
    "        # low_maes.append(low_mae)\n",
    "        # med_maes.append(med_mae)\n",
    "        # high_maes.append(high_mae)\n",
    "        # all_maes.append(all_mae)\n",
    "\n",
    "        # calculate precision and recall\n",
    "        low_precision, mid_precision, high_precision, low_recall, mid_recall, high_recall = precision_recall_at_k(predictions, k=10, threshold=3.5)\n",
    "        low_precisions.append(low_precision)\n",
    "        med_precisions.append(mid_precision)\n",
    "        high_precisions.append(high_precision)\n",
    "        all_precisions.append(np.mean([low_precision, mid_precision, high_precision]))\n",
    "\n",
    "        low_recalls.append(low_recall)\n",
    "        med_recalls.append(mid_recall)\n",
    "        high_recalls.append(high_recall)\n",
    "        all_recalls.append(np.mean([low_recall, mid_recall, high_recall]))\n",
    "\n",
    "        # get top-n recommendation counts\n",
    "        # top_n = get_top_n(predictions, n=10)\n",
    "        # for uid, user_ratings in top_n.items():\n",
    "        #     for (iid, _) in user_ratings:\n",
    "        #         df_item_dist.loc[iid, algo_names[i]] += 1\n",
    "        \n",
    "    # print('LowMS: ' + str(np.mean(low_maes)))\n",
    "    # print('MedMS: ' + str(np.mean(med_maes)))\n",
    "    # print('HighMS: ' + str(np.mean(high_maes)))\n",
    "    # print('All: ' + str(np.mean(all_maes)))\n",
    "    print('Low Precision: ' + str(np.mean(low_precisions)))\n",
    "    print('Med Precision: ' + str(np.mean(med_precisions)))\n",
    "    print('High Precision: ' + str(np.mean(high_precisions)))\n",
    "    print('All Precision: ' + str(np.mean(all_precisions)))\n",
    "    print('\\n')\n",
    "    print('Low Recall: ' + str(np.mean(low_recalls)))\n",
    "    print('Med Recall: ' + str(np.mean(med_recalls)))\n",
    "    print('High Recall: ' + str(np.mean(high_recalls)))\n",
    "    print('All Recall: ' + str(np.mean(all_recalls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNNBasic\n",
    "* Low Precision: 0.9938201267567784\n",
    "* Med Precision: 0.9955457707767221\n",
    "* High Precision: 0.994249268214902\n",
    "* All Precision: 0.9945383885828007\n",
    "\n",
    "* Low Recall: 0.39657141133273716\n",
    "* Med Recall: 0.4849721559767105\n",
    "* High Recall: 0.7247747820510398\n",
    "* All Recall: 0.5354394497868292\n",
    "\n",
    "KNNWithMeans\n",
    "* Low Precision: 0.9915877565683928\n",
    "* Med Precision: 0.9955473982119647\n",
    "* High Precision: 0.9942943911602035\n",
    "* All Precision: 0.9938098486468536\n",
    "\n",
    "* Low Recall: 0.39613806776898836\n",
    "* Med Recall: 0.48478519778438856\n",
    "* High Recall: 0.7247455270760168\n",
    "* All Recall: 0.5352229308764646\n",
    "\n",
    "NMF\n",
    "* Low Precision: 0.9925776793395326\n",
    "* Med Precision: 0.9952142907133863\n",
    "* High Precision: 0.9939831473956422\n",
    "* All Precision: 0.9939250391495204\n",
    "\n",
    "* Low Recall: 0.3960976163465185\n",
    "* Med Recall: 0.48452718107564297\n",
    "* High Recall: 0.7244137077574455\n",
    "* All Recall: 0.5350128350598691\n",
    "\n",
    "CoClustering\n",
    "* Low Precision: 0.9917593204269437\n",
    "* Med Precision: 0.9956996481792876\n",
    "* High Precision: 0.9943917621678924\n",
    "* All Precision: 0.9939502435913747\n",
    "\n",
    "* Low Recall: 0.3960974193955978\n",
    "* Med Recall: 0.48478724212146407\n",
    "* High Recall: 0.7246998161380069\n",
    "* All Recall: 0.535194825885023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: implicit in /home/alonso/.pyenv/versions/3.10.10/lib/python3.10/site-packages (0.7.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/alonso/.local/lib/python3.10/site-packages (from implicit) (1.26.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alonso/.local/lib/python3.10/site-packages (from implicit) (4.66.2)\n",
      "Requirement already satisfied: scipy>=0.16 in /home/alonso/.local/lib/python3.10/site-packages (from implicit) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl in /home/alonso/.local/lib/python3.10/site-packages (from implicit) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install implicit --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d69d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alonso/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import coo_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/myanime_600K.csv')\n",
    "df = df.rename(columns={'anime_id': 'item_id'})\n",
    "\n",
    "# Preprocess the data\n",
    "df['rating'] = df['rating'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Create a sparse matrix for ALS\n",
    "sparse_item_user = coo_matrix((df['rating'].astype(float),\n",
    "                               (df['item_id'], df['user_id'])))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a sparse matrix for ALS training\n",
    "sparse_item_user_train = coo_matrix((train_data['rating'].astype(float),\n",
    "                                     (train_data['item_id'], train_data['user_id'])))\n",
    "\n",
    "# Create a sparse matrix for ALS test\n",
    "sparse_item_user_test = coo_matrix((test_data['rating'].astype(float),\n",
    "                                    (test_data['item_id'], test_data['user_id'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fdb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alonso/.pyenv/versions/3.10.10/lib/python3.10/site-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "/home/alonso/.pyenv/versions/3.10.10/lib/python3.10/site-packages/implicit/utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.019881486892700195 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 20/20 [00:01<00:00, 19.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the ALS model\n",
    "model = AlternatingLeastSquares(factors=50, regularization=0.1, iterations=20)\n",
    "\n",
    "# Train the ALS model\n",
    "model.fit(sparse_item_user_train.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get MAE, Prevision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(model, user_id, sparse_item_user, N=10):\n",
    "    user_items = sparse_item_user.T.tocsr()\n",
    "    recommendations = model.recommend(user_id, user_items[user_id], N=N)\n",
    "    return recommendations[0], recommendations[1]\n",
    "\n",
    "\n",
    "# Define evaluation metrics\n",
    "def precision_at_k(r, k):\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "def average_precision(r):\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not idcg:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "# Create user-item interaction dictionaries for test data\n",
    "user_items_test = test_data.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, user_items_test, sparse_item_user, n=10):\n",
    "    mean_map_low = 0.\n",
    "    mean_map_med = 0.\n",
    "    mean_map_high = 0.\n",
    "    mean_ndcg_low = 0.\n",
    "    mean_ndcg_med = 0.\n",
    "    mean_ndcg_high = 0.\n",
    "    user_low = 0\n",
    "    user_med = 0\n",
    "    user_high = 0\n",
    "\n",
    "    user_items = sparse_item_user.T.tocsr()\n",
    "    \n",
    "    for user in user_items_test.keys():\n",
    "        rec_items, _ = recommend(model, user, user_items, N=n)\n",
    "        rel_vector = [1 if item in user_items_test[user] else 0 for item in rec_items]\n",
    "        if user in low_users.index:\n",
    "            mean_map_low += mean_average_precision([rel_vector])\n",
    "            mean_ndcg_low += ndcg_at_k(rel_vector, n)\n",
    "            user_low += 1\n",
    "        elif user in medium_users.index:\n",
    "            mean_map_med += mean_average_precision([rel_vector])\n",
    "            mean_ndcg_med += ndcg_at_k(rel_vector, n)\n",
    "            user_med += 1\n",
    "        else:\n",
    "            mean_map_high += mean_average_precision([rel_vector])\n",
    "            mean_ndcg_high += ndcg_at_k(rel_vector, n)\n",
    "            user_high += 1\n",
    "    mean_map_low /= user_low\n",
    "    mean_map_med /= user_med\n",
    "    mean_map_high /= user_high\n",
    "    mean_ndcg_low /= user_low\n",
    "    mean_ndcg_med /= user_med\n",
    "    mean_ndcg_high /= user_high\n",
    "    print(user_low, user_med, user_high)\n",
    "    \n",
    "    return mean_map_low, mean_map_med, mean_map_high, mean_ndcg_low, mean_ndcg_med, mean_ndcg_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179 1203 1060\n",
      "MAP Low: 0.134152199424676\n",
      "MAP Med: 0.12857068637928976\n",
      "MAP High: 0.11564153439153442\n",
      "NDCG Low: 0.2258431700285121\n",
      "NDCG Med: 0.22331426922678385\n",
      "NDCG High: 0.20496220492654404\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform the evaluation\n",
    "mean_map_low, mean_map_med, mean_map_high, mean_ndcg_low, mean_ndcg_med, mean_ndcg_high = evaluate_model(model, user_items_test, sparse_item_user_test, n=10)\n",
    "print('MAP Low: ' + str(mean_map_low))\n",
    "print('MAP Med: ' + str(mean_map_med))\n",
    "print('MAP High: ' + str(mean_map_high))\n",
    "print('NDCG Low: ' + str(mean_ndcg_low))\n",
    "print('NDCG Med: ' + str(mean_ndcg_med))\n",
    "print('NDCG High: ' + str(mean_ndcg_high))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
